{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d02034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ldoddi/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import torch.nn.utils.prune\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95f0ad7-f6d7-43fc-b430-8c1836b8b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19f02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "\n",
    "    tweet2 = re.sub(r'^RT[\\s]+','', tweet)\n",
    "   \n",
    "    #remove hyperlinks\n",
    "    tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
    "    \n",
    "    #remove hashtag by removing the hast #sign from the word\n",
    "    tweet2 = re.sub(r'#','',tweet2)\n",
    "    \n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    #convert into lower case\n",
    "    tweets_clean = tweet.lower()\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d3c0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/sentiment_label_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec35773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the tweets \n",
    "data[\"tweets\"] = data[\"text\"].map(lambda x : process_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4af982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54750    holy shit. just saw the surgeon who did my las...\n",
       "73756            thoughts are with the af crew &amp;  pax \n",
       "93620    i just got a sweet little add on for fire fox....\n",
       "79969    @momspective that smartfood better be good   o...\n",
       "74747    is bored in work! and its raining again! team ...\n",
       "35591    melanie, i cant explain in words how excited i...\n",
       "59                        @leslie_knope u will beat oprah \n",
       "36260    it's just one of those days where i want my fa...\n",
       "33577                                i have a stiff neck  \n",
       "17863    wow, a high of 64 today. i love this weather i...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweets\"].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014c6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.index.values, data.sentiment.values, test_size = 0.333, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d0ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"data_type\"] = ['not_set']*data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aabe698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[X_train, 'data_type'] = 'train'\n",
    "data.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e5fe079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tweets</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65308</th>\n",
       "      <td>1087359</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello all ... (I won't do this often) ... Just...</td>\n",
       "      <td>hello all ... (i won't do this often) ... just...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17388</th>\n",
       "      <td>1411890</td>\n",
       "      <td>1</td>\n",
       "      <td>gotta work... off for now.</td>\n",
       "      <td>gotta work... off for now.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32442</th>\n",
       "      <td>941765</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, shoot. And I don't have a ride to or from ...</td>\n",
       "      <td>oh, shoot. and i don't have a ride to or from ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28526</th>\n",
       "      <td>1364753</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting Prepared for the next 3 weeks, School ...</td>\n",
       "      <td>getting prepared for the next 3 weeks, school ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28704</th>\n",
       "      <td>658450</td>\n",
       "      <td>0</td>\n",
       "      <td>Tears  x</td>\n",
       "      <td>tears  x</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83822</th>\n",
       "      <td>1000349</td>\n",
       "      <td>1</td>\n",
       "      <td>@helga_hansen http://twitpic.com/5of2w - Oh my...</td>\n",
       "      <td>@helga_hansen http://twitpic.com/5of2w - oh my...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>708697</td>\n",
       "      <td>0</td>\n",
       "      <td>so bored! I is housebound for the weekend  x</td>\n",
       "      <td>so bored! i is housebound for the weekend  x</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13611</th>\n",
       "      <td>37726</td>\n",
       "      <td>0</td>\n",
       "      <td>@MsHollyOlly3 hahah i have journalism</td>\n",
       "      <td>@mshollyolly3 hahah i have journalism</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70579</th>\n",
       "      <td>920192</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy Mothers Day to all Mothers in America, o...</td>\n",
       "      <td>happy mothers day to all mothers in america, o...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63222</th>\n",
       "      <td>1058361</td>\n",
       "      <td>1</td>\n",
       "      <td>LOOOOK LOOK!!!! 50 folllowers!!!! yayyy!!!! th...</td>\n",
       "      <td>looook look!!!! 50 folllowers!!!! yayyy!!!! th...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  sentiment  \\\n",
       "65308     1087359          1   \n",
       "17388     1411890          1   \n",
       "32442      941765          1   \n",
       "28526     1364753          1   \n",
       "28704      658450          0   \n",
       "83822     1000349          1   \n",
       "7279       708697          0   \n",
       "13611       37726          0   \n",
       "70579      920192          1   \n",
       "63222     1058361          1   \n",
       "\n",
       "                                                    text  \\\n",
       "65308  Hello all ... (I won't do this often) ... Just...   \n",
       "17388                        gotta work... off for now.    \n",
       "32442  Oh, shoot. And I don't have a ride to or from ...   \n",
       "28526  Getting Prepared for the next 3 weeks, School ...   \n",
       "28704                                           Tears  x   \n",
       "83822  @helga_hansen http://twitpic.com/5of2w - Oh my...   \n",
       "7279        so bored! I is housebound for the weekend  x   \n",
       "13611             @MsHollyOlly3 hahah i have journalism    \n",
       "70579  Happy Mothers Day to all Mothers in America, o...   \n",
       "63222  LOOOOK LOOK!!!! 50 folllowers!!!! yayyy!!!! th...   \n",
       "\n",
       "                                                  tweets data_type  \n",
       "65308  hello all ... (i won't do this often) ... just...      test  \n",
       "17388                        gotta work... off for now.      train  \n",
       "32442  oh, shoot. and i don't have a ride to or from ...     train  \n",
       "28526  getting prepared for the next 3 weeks, school ...     train  \n",
       "28704                                           tears  x      test  \n",
       "83822  @helga_hansen http://twitpic.com/5of2w - oh my...     train  \n",
       "7279        so bored! i is housebound for the weekend  x      test  \n",
       "13611             @mshollyolly3 hahah i have journalism      train  \n",
       "70579  happy mothers day to all mothers in america, o...     train  \n",
       "63222  looook look!!!! 50 folllowers!!!! yayyy!!!! th...     train  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a05855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the data\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74be121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    data[data.data_type=='train'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "                            \n",
    ")\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    data[data.data_type=='test'].text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "                            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e070a77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array = data[\"sentiment\"].unique()\n",
    "label_dict = dict(enumerate(label_array.flatten(), 1))\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b1eeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(data[data.data_type=='train'].sentiment.values)\n",
    "\n",
    "# test dataset\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(data[data.data_type=='test'].sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99e70ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing datasets into train and test\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_test = TensorDataset(input_ids_test,attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c745ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up BERT pretrained model\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a91f2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels = len(label_dict),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b28efb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e5414d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "   dataset_train, \n",
    "   sampler=RandomSampler(dataset_train),\n",
    "   batch_size=batch_size)\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "   dataset_test, \n",
    "   sampler=RandomSampler(dataset_test),\n",
    "   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e80ff47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4951731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting hyper parameters\n",
    "optimizer = torch.optim.AdamW(\n",
    " model.parameters(),\n",
    "    lr=1e-5,\n",
    "    eps=1e-8   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19ff2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    " optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58abca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb38e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bab0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    lables_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cecefd",
   "metadata": {},
   "source": [
    "## Creating our Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3851e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3a21fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5440e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_test):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_test:\n",
    "        \n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]\n",
    "                 }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_val_total += loss.item()\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "        \n",
    "        loss_val_avg = loss_val_total/len(dataloader_test)\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "        \n",
    "        return loss_val_avg, predictions, true_vals\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be5c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3b2844302b4596bca8e05805d84fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a8b2e40a594c538260f679b7eab0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1:   0%|          | 0/16675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader_train,\n",
    "                        desc='epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                        disable=False)\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids' : batch[0],\n",
    "            'attention_mask' : batch[1],\n",
    "            'labels' : batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss= outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "                                   \n",
    "    print('training_loss : ',(loss.item()/len(batch)))    \n",
    "    torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')    \n",
    "    tqdm.write('\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_test)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (weighted): {val_f1}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a052f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
